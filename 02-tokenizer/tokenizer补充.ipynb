{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分词背后的原理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs:\n",
      " tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
      "          2026,  2878,  2166,  1012]])\n",
      "Logits:\n",
      " tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "checkpoint = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "\n",
    "# 分词：['i', \"'\", 've', 'been', 'waiting', 'for', 'a', 'hugging', '##face', 'course', 'my', 'whole', 'life', '.']\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "# 这里[ids]是因为 所有的神经网络模型都只接受批 (batch) 数据作为输入，即使只输入一段文本，也需要先将它组成只包含一个样本的 batch\n",
    "#  ，更多情况下送入的是包含多段文本的 batch：batched_ids = [ids, ids, ids, ...]\n",
    "input_ids = torch.tensor([ids])\n",
    "print(\"Input IDs:\\n\", input_ids)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(\"Logits:\\n\", output.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实际应用中，我们应该直接使用分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs {'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "output SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "tensor([[0.0402, 0.9598]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "checkpoint = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "# 批量处理。padding=True 表示在处理输入序列时，会自动将序列填充到相同的长度。\n",
    "inputs = tokenizer(sequence,  return_tensors='pt', padding=True)\n",
    "print(\"inputs\", inputs)\n",
    "\n",
    "output = model(**inputs)\n",
    "print(\"output\", output)\n",
    "\n",
    "# tensor([[-1.5607,  1.6123]], grad_fn=<AddmmBackward0>)\n",
    "# 模型的输出通常是一个张量（tensor），它包含了模型对输入数据的预测结果。\n",
    "# output.logits 的结果 tensor([[-1.5607, 1.6123]], grad_fn=<AddmmBackward0>) 是一个包含两个元素的张量，这两个元素分别表示模型对输入序列的两个类别的预测分数。\n",
    "# -1.5607 是模型对第一个类别的预测分数。；1.6123 是模型对第二个类别的预测分数。\n",
    "\n",
    "# 为了得到最终的预测结果，通常需要对这些 logits 进行 softmax 操作，将它们转换为概率分布。\n",
    "# softmax 操作会将这些分数转换为介于 0 和 1 之间的值，并且所有值的和为 1。这样，我们就可以更容易地解释模型对每个类别的预测概率。\n",
    "output.logits\n",
    "\n",
    "\n",
    "# softmax 操作\n",
    "import torch.nn.functional as F\n",
    "probs = F.softmax(output.logits, dim=-1)\n",
    "print(probs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 句子对的处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2023, 2003, 1996, 2034, 6251, 1015, 1012,  102, 2023, 2003, 1996,\n",
      "         2034, 6251, 1016, 1012,  102],\n",
      "        [ 101, 2117, 6251, 1015, 1012,  102, 2117, 6251, 1016, 1012,  102,    0,\n",
      "            0,    0,    0,    0,    0],\n",
      "        [ 101, 3231, 2487,  102, 3231, 2475,  102,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "torch.Size([3, 17])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "sentence1_list = [\"This is the first sentence 1.\", \"second sentence 1.\",\"test1\"]\n",
    "sentence2_list = [\"This is the first sentence 2.\", \"second sentence 2.\", \"test2\"]\n",
    "\n",
    "\n",
    "# 句子对配对时候是要 sentence1_list 中的第一个句子与 sentence2_list 中的第一个句子配对，\n",
    "inputs = tokenizer(\n",
    "    sentence1_list,\n",
    "    sentence2_list,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "print(inputs)\n",
    "\n",
    "# torch.Size([3, 17]): 标识有三句子，每个句子最大长度17(包括填充)\n",
    "print(inputs['input_ids'].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
